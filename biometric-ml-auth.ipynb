{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fbd39a",
   "metadata": {},
   "source": [
    "# Biometric Authentication System\n",
    "\n",
    "This notebook implements a comprehensive biometric authentication system that combines **voice recognition** and **facial recognition** technologies to provide secure, passwordless authentication.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Traditional authentication methods relying on emails and passwords are increasingly vulnerable to security breaches. This biometric system offers a more secure alternative by utilizing:\n",
    "\n",
    "- **Voice Biometrics**: Analyzes unique vocal characteristics and speech patterns\n",
    "- **Facial Recognition**: Identifies individuals based on facial features and geometry\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Multi-modal Authentication**: Combines voice and visual biometrics for enhanced security\n",
    "- **Real-time Processing**: Live capture and analysis of voice and facial data\n",
    "- **Machine Learning Integration**: Uses TensorFlow/Keras models for pattern recognition\n",
    "- **No Password Required**: Eliminates the need for traditional credentials\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "- **Computer Vision**: OpenCV for image processing and facial detection\n",
    "- **Audio Processing**: PyAudio for real-time voice capture and analysis\n",
    "- **Machine Learning**: TensorFlow/Keras for deep learning models\n",
    "- **Feature Extraction**: MFCC (Mel-frequency cepstral coefficients) for voice analysis\n",
    "- **Biometric Matching**: Gaussian Mixture Models for voice pattern recognition\n",
    "\n",
    "This system provides a robust, user-friendly authentication solution that leverages the uniqueness of human biometric traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae0085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "\n",
    "import pyaudio\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import wave\n",
    "from scipy.io.wavfile import read\n",
    "#from sklearn.mixture import GMM \n",
    "from sklearn.mixture import GaussianMixture \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baba1a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5e34c",
   "metadata": {},
   "source": [
    "# Audio Processing\n",
    "\n",
    "## Voice Biometric Feature Extraction\n",
    "\n",
    "This section focuses on extracting and processing acoustic features from voice recordings for biometric authentication. The audio processing pipeline converts raw voice data into machine-readable features that capture unique vocal characteristics.\n",
    "\n",
    "### MFCC Feature Extraction\n",
    "\n",
    "The system uses **Mel-Frequency Cepstral Coefficients (MFCC)** as the primary feature extraction technique:\n",
    "\n",
    "- **12 MFCC coefficients** are extracted from each audio frame\n",
    "- **Frame size**: 25ms with 10ms overlap for detailed temporal analysis\n",
    "- **Energy component** is appended to capture voice intensity patterns\n",
    "- **FFT size**: 2048 points for high-frequency resolution\n",
    "\n",
    "### Delta Features\n",
    "\n",
    "To capture temporal dynamics in speech patterns, the system computes **delta features**:\n",
    "\n",
    "- **First-order derivatives** of MFCC coefficients\n",
    "- Represents rate of change in spectral characteristics\n",
    "- Combined with original MFCC features for comprehensive voice modeling\n",
    "\n",
    "### Feature Normalization\n",
    "\n",
    "- **Z-score normalization** applied to ensure consistent feature scaling\n",
    "- Removes bias from different recording conditions\n",
    "- Improves model convergence and matching accuracy\n",
    "\n",
    "### Voice Pattern Modeling\n",
    "\n",
    "The extracted features will be used to train **Gaussian Mixture Models (GMM)** that learn individual voice patterns, enabling accurate speaker identification and verification for secure authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0932f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta(array):\n",
    "    rows, cols = array.shape\n",
    "    deltas = np.zeros((rows, cols))\n",
    "    N = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while len(index) < 2 and j < N+1:\n",
    "            if i-j >= 0:\n",
    "                index.append(i-j)\n",
    "            if i+j < rows:\n",
    "                index.append(i+j)\n",
    "            j+=1\n",
    "        if len(index) == 2:\n",
    "            deltas[i] = (array[index[1]] - array[index[0]]) / (2*N)\n",
    "        elif len(index) == 1:\n",
    "            deltas[i] = (array[index[0]] - array[i]) / N\n",
    "    return deltas\n",
    "\n",
    "\n",
    "#convert audio to mfcc features\n",
    "def extract_features(audio, rate):    \n",
    "    mfcc_feat = mfcc.mfcc(audio, rate, 0.025, 0.01, 12, appendEnergy=True, nfft=2048)\n",
    "    mfcc_feat = preprocessing.scale(mfcc_feat)\n",
    "    delta = calculate_delta(mfcc_feat)\n",
    "\n",
    "    #combining both mfcc features and delta\n",
    "    combined = np.hstack((mfcc_feat, delta)) \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b11c5f",
   "metadata": {},
   "source": [
    "# User Registration\n",
    "\n",
    "## Adding New Users to the System\n",
    "\n",
    "This section covers the process of registering new users in the biometric authentication system. The registration process involves capturing and storing biometric data for both voice and facial recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ace55bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:06:28.746 Python[27820:304246] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/hq/hty7b3f54q99qj56brbxr0p00000gn/T/org.python.python.savedState\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n",
      "Image saved successfully in database!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create a directory to store captured images\n",
    "dir_name = 'saved_images'\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(dir_name)\n",
    "\n",
    "# Get name of the person to capture images\n",
    "name = input(\"Enter your name: \")\n",
    "\n",
    "# Create a directory with the name of the person\n",
    "person_dir = os.path.join(dir_name, name)\n",
    "if not os.path.exists(person_dir):\n",
    "    os.makedirs(person_dir)\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the width and height of the capture window\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "# Capture frames continuously\n",
    "while True:\n",
    "    # Capture a frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret or frame is None:\n",
    "        print(\"Failed to capture frame from camera. Exiting...\")\n",
    "        break\n",
    "\n",
    "    # Display instructions in the frame\n",
    "    cv2.putText(frame, \"Press 'q' to exit\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    cv2.putText(frame, \"Press 's' to save image\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    # Exit the camera if 'q' is pressed\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    # Save the image if 's' is pressed\n",
    "    if key == ord('s'):\n",
    "        # Generate a unique filename for the image\n",
    "        filename = os.path.join(person_dir, name + \"_\" + str(len(os.listdir(person_dir))+1) + \".jpg\")\n",
    "\n",
    "        # Save the image\n",
    "        cv2.imwrite(filename, frame)\n",
    "        print(\"Image saved successfully in database!\")\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24c91c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known face encodings: [array([-0.15630172, -0.03808632,  0.01052278, -0.0074402 , -0.06061751,\n",
      "        0.02011276,  0.0702951 , -0.09100263,  0.16502056, -0.10435909,\n",
      "        0.19622302, -0.06010798, -0.17718334, -0.11288489, -0.03114074,\n",
      "        0.12462305, -0.11458257, -0.16932786, -0.0450679 , -0.12066258,\n",
      "        0.03476317, -0.01583534, -0.00969218,  0.10023776, -0.16187677,\n",
      "       -0.38878292, -0.05838905, -0.17021155,  0.03976464, -0.07268727,\n",
      "       -0.01359243,  0.1020198 , -0.21776104, -0.00594426, -0.03758461,\n",
      "        0.05819738,  0.07864693,  0.01889201,  0.12699898,  0.05223377,\n",
      "       -0.17250392, -0.0886682 ,  0.02712412,  0.26270956,  0.16132922,\n",
      "        0.04599509,  0.01933765, -0.00728455,  0.01621879, -0.26146719,\n",
      "        0.0645002 ,  0.07115059,  0.13612103, -0.01617012,  0.0645465 ,\n",
      "       -0.16215117, -0.05950534,  0.02541297, -0.14464839,  0.08522309,\n",
      "       -0.02649183, -0.01096546, -0.04645266, -0.03601773,  0.28627971,\n",
      "        0.11091712, -0.11283754, -0.04810533,  0.20649153, -0.18180141,\n",
      "       -0.03186268, -0.00460087, -0.10022017, -0.17865801, -0.34687269,\n",
      "        0.07842468,  0.43582195,  0.16763328, -0.18496603,  0.04904518,\n",
      "       -0.13274789,  0.05641838,  0.08597424,  0.06949793, -0.10953087,\n",
      "        0.05420751, -0.0539269 ,  0.03211036,  0.13409261,  0.04000967,\n",
      "       -0.0693397 ,  0.18541831, -0.07750669,  0.02189902,  0.02088445,\n",
      "        0.01094098, -0.04039561, -0.00887402, -0.08856624, -0.02650776,\n",
      "        0.0315222 , -0.02269743,  0.06212535,  0.13657102, -0.23540811,\n",
      "        0.08034587, -0.03547282, -0.06023504,  0.01928546,  0.0917043 ,\n",
      "       -0.1176305 , -0.04047884,  0.11037222, -0.27835158,  0.13093795,\n",
      "        0.2085593 ,  0.01712476,  0.18903877,  0.06582604,  0.03454137,\n",
      "        0.01767835, -0.01695076, -0.1562493 , -0.00989541,  0.08454643,\n",
      "       -0.0942824 ,  0.01018184,  0.05167031])]\n",
      "Known face names: ['Siddharth']\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty arrays to store the face encodings and names\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "path = \"saved_images\"\n",
    "\n",
    "# Set the path to the folder containing the images of the particular person\n",
    "for file in os.listdir(path):\n",
    "    \n",
    "    person_path = os.path.join(path, file)\n",
    "    \n",
    "    # Get the list of image files in the folder\n",
    "    image_files = os.listdir(person_path)\n",
    "\n",
    "    # Check if there is at least one image file in the folder\n",
    "    if len(image_files) > 0:\n",
    "        # Select the first image file in the folder\n",
    "        image_file = image_files[0]\n",
    "    \n",
    "        # Load the image file\n",
    "        image = face_recognition.load_image_file(os.path.join(person_path, image_file))\n",
    "    \n",
    "        # Extract the face encoding from the image\n",
    "        encoding = face_recognition.face_encodings(image)[0]\n",
    "    \n",
    "        # Extract the name of the person from the filename\n",
    "        name = os.path.splitext(file)[0]\n",
    "    \n",
    "        # Append the face encoding and name to their respective arrays\n",
    "        known_face_encodings.append(encoding)\n",
    "        known_face_names.append(name)\n",
    "\n",
    "# Print the arrays of known face encodings and their names\n",
    "print(\"Known face encodings:\", known_face_encodings)\n",
    "print(\"Known face names:\", known_face_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b9132c",
   "metadata": {},
   "source": [
    "# Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e734f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recording... say hello authentication for authentication\n",
      "finished recording\n",
      "None\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "global identity\n",
    "identity = None\n",
    "def recognize_voice():\n",
    "    global identity\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    RATE = 44100\n",
    "    CHUNK = 1024\n",
    "    RECORD_SECONDS = 3\n",
    "    FILENAME = \"./voice.wav\"\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Check if CHANNELS is a valid value for your microphone\n",
    "    try:\n",
    "        stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                            rate=RATE, input=True,\n",
    "                            frames_per_buffer=CHUNK)\n",
    "    except OSError as e:\n",
    "        print(f\"Error opening audio stream: {e}\")\n",
    "        print(\"Please check your microphone and CHANNELS setting.\")\n",
    "        audio.terminate()\n",
    "        return\n",
    "\n",
    "    print(\"recording... say hello authentication for authentication\")\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"finished recording\")\n",
    "\n",
    "    # stop Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "recognize_voice()\n",
    "print(identity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4931ee",
   "metadata": {},
   "source": [
    "# FACE AND VOICE AUTHENTICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732f4399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep Your face infront of the camera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:08:15.783 Python[27891:307152] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/hq/hty7b3f54q99qj56brbxr0p00000gn/T/org.python.python.savedState\n"
     ]
    }
   ],
   "source": [
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True    \n",
    "\n",
    "print(\"Keep Your face infront of the camera\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640)\n",
    "cap.set(4, 480)\n",
    "    \n",
    "    \n",
    "time.sleep(1.0)\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    curr_time = time.time()\n",
    "            \n",
    "    _, frame = cap.read()\n",
    "        \n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        \n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        \n",
    "    face=face_recognition.face_locations(rgb_small_frame)\n",
    "        \n",
    "    if len(face) == 1:\n",
    "            \n",
    "        if process_this_frame:\n",
    "                \n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame.astype(np.uint8), face_locations)\n",
    "                \n",
    "            face_name = []\n",
    "                \n",
    "            for face_encoding in face_encodings:\n",
    "                    \n",
    "                # See if the face is a match for the known face(s)\n",
    "                matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                facename=\"Unknown\"\n",
    "                    # # If a match was found in known_face_encodings, just use the first one.\n",
    "                    #if True in matches:\n",
    "                    #first_match_index = matches.index(True)\n",
    "                    #name = known_face_names[first_match_index]\n",
    "                    # Or instead, use the known face with the smallest distance to the new face\n",
    "                    \n",
    "                face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "            \n",
    "                if matches[best_match_index]:\n",
    "                    facename = known_face_names[best_match_index]\n",
    "                    face_names.append(facename)\n",
    "                            \n",
    "                 # if min dist is less then threshold value \n",
    "                 # and both face and voice matched than unlock the door\n",
    "                \n",
    "        process_this_frame = not process_this_frame\n",
    "        # Display the results\n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "              # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 0), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            \n",
    "            if facename == identity: \n",
    "                cv2.putText(frame, facename +\" Authenticaation successful !welcome\", (left + 6, bottom - 6), font, 0.5, (0, 255, 0), 1)\n",
    "            elif facename==\"Unknown\":\n",
    "                cv2.putText(frame, facename+\" Face not registered,! Unsuccessful\", (left + 6, bottom - 6), font, 0.5, (0, 0, 255), 1)\n",
    "            elif identity==\"unknown\":\n",
    "                cv2.putText(frame, facename+\" Voice not recognized , !Try again.....\", (left + 6, bottom - 6), font, 0.5, (0, 0, 255), 1)\n",
    "            else: \n",
    "                cv2.putText(frame, facename+\" Voice identity not matching with face !Try again....\", (left + 6, bottom - 6), font, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        # Hit 'q' on the keyboard to quit!\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release handle to the webcam\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if len(face) == 0:\n",
    "    print('There was no face found in the frame. Try again...')\n",
    "                \n",
    "\n",
    "elif len(face) > 1:\n",
    "    print(\"More than one faces found. Try again...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
